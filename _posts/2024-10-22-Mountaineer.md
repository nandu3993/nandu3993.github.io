---
title: Mountaineer
date: 2024-10-22
tags: [tryhackme, ctf]
author: nandakishor
---

## How to Efficiently Serve an LLM

LLMs, or **Large** Language Models, are named so because they can range from tens to hundreds of billions of parameters. Their utility is clear, as LLMs are setting new records on various benchmarks and now often match or exceed human performance in multiple tasks Consequently, many companies are eager to deploy them in production. However, due to the unprecedented size of LLMs, there are significant challenges in serving them, such as slow token generation (tokens/second), memory limits for loading model parameters, KV cache (explained later), compute limits, and more. In this article, we will cover several recent ideas to help set up a robust LLM serving system.


hi
![alt text](<../assets/images/mountaineer/Screenshot 2024-10-22 141106.png>)
